# PySpark
PySpark is the Python API for Apache Spark, used for big data processing. To get started, the key concepts include:

- **RDD (Resilient Distributed Dataset)**: The fundamental data structure in Spark, created using `parallelize()` or from external data sources. RDDs support operations like `map()`, `filter()`, and `reduce()` for data transformations.

- **DataFrames**: Higher-level API built on RDDs that offers SQL-like functionality. You can perform operations such as `select()`, `filter()`, and `where()` to manipulate and query data.

- **groupBy()**: Used to group data based on specific columns and apply aggregate functions like `sum()` or `count()`.

By understanding these basics, you can start working with PySpark for efficient distributed data processing.
